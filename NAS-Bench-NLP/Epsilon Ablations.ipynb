{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epsilon ablation studies\n",
    "In this notebook we present the code used for ablation studies published in our paper. **By default, precomputed results will be loaded** and printed out. **If you wish to run the ablations from scratch**, change the `results_dir` directory name from `release_results` to any other name you please."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import data\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import seaborn as sns\n",
    "from tqdm import trange\n",
    "from prettytable import PrettyTable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils import batchify\n",
    "from argparse import Namespace\n",
    "from model import AWDRNNModel\n",
    "from train import train, evaluate\n",
    "from utils import get_batch\n",
    "from multilinear import MultiLinear\n",
    "from custom_rnn import CustomRNNCell, CustomRNN\n",
    "from weight_drop import ParameterListWeightDrop\n",
    "from epsilon_utils import prepare_seed, prepare_recepies, compute_stats, plot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to a different name if you would like to run ablation tests from scratch\n",
    "results_dir = './release_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a fixed batch of data\n",
    "file_list=os.listdir(\"train_logs_single_run/\")\n",
    "log_dflt = json.load(open('train_logs_single_run/' + file_list[0], 'r'))\n",
    "args = Namespace(**log_dflt)\n",
    "corpus = data.Corpus(args.data)\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "batch_size = 256\n",
    "\n",
    "train_eval_data = batchify(corpus.train, batch_size, args, \"cpu\")\n",
    "x, _ = get_batch(train_eval_data, 0, args, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3631"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight ablations\n",
    "\n",
    "In this ablation study we verify how much does the performance of epsilon metric change depending on the chosen weights. Since we can only run epsilon computation for NAS-Bench-NLP only on CPU, we perform this study only for 1000 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weight_range = [1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "it = 0\n",
    "for (weight_l, weight_h) in itertools.product(weight_range, weight_range):\n",
    "    \n",
    "    if weight_h>weight_l:\n",
    "        save_dir = '{}/NLP/Ablation/Weights/Weight_{}_{}/BS{}/'.format(results_dir, weight_l, weight_h, batch_size)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        if os.path.exists(save_dir+'Data'):\n",
    "            # Load precomputed results\n",
    "            data_file = open(save_dir+'Data','rb')\n",
    "            data = pkl.load(data_file)\n",
    "            score = data[\"score\"]\n",
    "            accs = data[\"accs\"]\n",
    "            nparams = data[\"nparams\"]\n",
    "        else:\n",
    "            accs = []\n",
    "            nparams = []\n",
    "            score = []\n",
    "            for i in trange(5000):\n",
    "                file = file_list[i]\n",
    "                log = json.load(open('train_logs_single_run/' + file, 'r'))\n",
    "                args = Namespace(**log)\n",
    "\n",
    "                # Build the model\n",
    "                network = AWDRNNModel(args.model,\n",
    "                                      ntokens,\n",
    "                                      args.emsize,\n",
    "                                      args.nhid,\n",
    "                                      args.nlayers,\n",
    "                                      args.dropout,\n",
    "                                      args.dropouth,\n",
    "                                      args.dropouti,\n",
    "                                      args.dropoute,\n",
    "                                      args.wdrop,\n",
    "                                      args.tied,\n",
    "                                      args.recepie,\n",
    "                                      verbose=False)\n",
    "                preds = []\n",
    "                for weight in [weight_l, weight_h]:\n",
    "                    # Initialize\n",
    "                    prepare_seed(21)\n",
    "                    def initialize_resnet(m):\n",
    "                        if type(m)==MultiLinear:\n",
    "                            for par in m.weights_raw:\n",
    "                                nn.init.constant_(par, weight)\n",
    "                        elif type(m)==CustomRNNCell:\n",
    "                            for par in m.parameters():\n",
    "                                nn.init.constant_(par, weight)\n",
    "                        elif type(m)==nn.modules.linear.Linear:\n",
    "                            nn.init.constant_(m.weight, weight)\n",
    "                        elif type(m)==nn.modules.container.ParameterList:\n",
    "                            for par in m.parameters():\n",
    "                                nn.init.constant_(par, weight)\n",
    "                        elif type(m)==CustomRNN:\n",
    "                            initialize_resnet(m.cell)\n",
    "                        elif type(m)==ParameterListWeightDrop:\n",
    "                            initialize_resnet(m.module)\n",
    "                        elif type(m)==nn.modules.container.ModuleDict:\n",
    "                            for sub_m in m:\n",
    "                                initialize_resnet(sub_m)\n",
    "                        elif type(m)==nn.modules.container.ModuleList:\n",
    "                            for sub_m in m:\n",
    "                                initialize_resnet(sub_m)\n",
    "                        elif type(m)==AWDRNNModel:\n",
    "                            initialize_resnet(m.rnns)\n",
    "\n",
    "                    network.apply(initialize_resnet)\n",
    "                    network.eval()\n",
    "                    hidden = network.init_hidden(batch_size, weight)\n",
    "                    # Take care of embedding is not constant\n",
    "                    nn.init.uniform_(network.encoder.weight, 0, 1)\n",
    "                    _, _, raw_output, _ = network(x, hidden=hidden, return_h=True)\n",
    "                    pred = raw_output[-1][:,:,0].flatten()\n",
    "                    pred = pred.numpy()\n",
    "                    pred_min = np.nanmin(pred)\n",
    "                    pred_max = np.nanmax(pred)\n",
    "                    pred_norm = (pred - pred_min)/(pred_max - pred_min)\n",
    "                    preds.append(pred_norm)\n",
    "\n",
    "                # Compute the score\n",
    "                preds = np.array(preds)\n",
    "                preds[np.where(preds==0)] = np.nan\n",
    "                mae = np.nanmean(np.abs(preds[0,:]-preds[1,:]))\n",
    "                mean = np.nanmean(preds)\n",
    "\n",
    "                score.append(mae/mean)\n",
    "                nparams.append(args.num_params)\n",
    "                try:\n",
    "                    accs.append(log['test_losses'][-1])\n",
    "                except:\n",
    "                    accs.append(np.nan) \n",
    "\n",
    "            save_dic = {}\n",
    "            save_dic[\"score\"] = score\n",
    "            save_dic[\"accs\"] = accs\n",
    "            save_dic[\"nparams\"] = nparams\n",
    "            pkl.dump(save_dic, open(save_dir + \"Data\", \"wb\"))\n",
    "            \n",
    "        if it==0:\n",
    "            # Make the table\n",
    "            headers = [\"Weights\", \"Archs\", \"Spearman (global)\", \"Spearman (top-10%)\", \"Kendall (global)\", \"Kendall (top-10%)\", \"Top-10%/top-10%\", \"Top-64/top-5%\"]\n",
    "            table = PrettyTable(headers)\n",
    "        stats, remain = compute_stats(score, accs)\n",
    "        stats_print = [\"[{}, {}]\".format(weight_l, weight_h), remain] + stats        \n",
    "        table.add_row(stats_print)\n",
    "        it+=1\n",
    "        \n",
    "#         score_plot = np.array(score)[np.array(score)>0.00001]\n",
    "#         accs_plot = np.array(accs)[np.array(score)>0.00001]\n",
    "#         nparams_plot = np.array(nparams)[np.array(score)>0.00001]\n",
    "        \n",
    "#         plot_results(score=score_plot,\n",
    "#                      accs=accs_plot,\n",
    "#                      nparams=nparams_plot,\n",
    "#                      top10=False,\n",
    "#                      log_scale=True,\n",
    "#                      save_dir=save_dir,\n",
    "#                      save_name=\"Epsilon_{}_{}\".format(weight_l, weight_h))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1e-07, 1e-06]</td>\n",
       "      <td>1663</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1e-07, 1e-05]</td>\n",
       "      <td>2211</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.82</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1e-07, 0.0001]</td>\n",
       "      <td>2325</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.53</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1e-07, 0.001]</td>\n",
       "      <td>2304</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.39</td>\n",
       "      <td>3.03</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1e-07, 0.01]</td>\n",
       "      <td>914</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3.30</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[1e-07, 0.1]</td>\n",
       "      <td>465</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.18</td>\n",
       "      <td>14.89</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[1e-07, 1]</td>\n",
       "      <td>395</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.03</td>\n",
       "      <td>12.50</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[1e-06, 1e-05]</td>\n",
       "      <td>2806</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[1e-06, 0.0001]</td>\n",
       "      <td>3234</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.41</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[1e-06, 0.001]</td>\n",
       "      <td>3211</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.24</td>\n",
       "      <td>4.67</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[1e-06, 0.01]</td>\n",
       "      <td>1280</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.12</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[1e-06, 0.1]</td>\n",
       "      <td>696</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>14.29</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[1e-06, 1]</td>\n",
       "      <td>578</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.13</td>\n",
       "      <td>12.07</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[1e-05, 0.0001]</td>\n",
       "      <td>3876</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[1e-05, 0.001]</td>\n",
       "      <td>3879</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>0.09</td>\n",
       "      <td>3.87</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[1e-05, 0.01]</td>\n",
       "      <td>1625</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.46</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.31</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[1e-05, 0.1]</td>\n",
       "      <td>876</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.20</td>\n",
       "      <td>10.23</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[1e-05, 1]</td>\n",
       "      <td>641</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.24</td>\n",
       "      <td>10.94</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0001, 0.001]</td>\n",
       "      <td>3979</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>0.10</td>\n",
       "      <td>4.04</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0001, 0.01]</td>\n",
       "      <td>1922</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.26</td>\n",
       "      <td>3.63</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0001, 0.1]</td>\n",
       "      <td>918</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.14</td>\n",
       "      <td>11.96</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0001, 1]</td>\n",
       "      <td>663</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.24</td>\n",
       "      <td>10.45</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.001, 0.01]</td>\n",
       "      <td>1931</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.26</td>\n",
       "      <td>2.59</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.001, 0.1]</td>\n",
       "      <td>913</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.17</td>\n",
       "      <td>8.70</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.001, 1]</td>\n",
       "      <td>663</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>7.46</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.01, 0.1]</td>\n",
       "      <td>797</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.01, 1]</td>\n",
       "      <td>651</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.06</td>\n",
       "      <td>9.23</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.1, 1]</td>\n",
       "      <td>639</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.17</td>\n",
       "      <td>10.94</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0     1      2      3      4      5      6     7\n",
       "0    [1e-07, 1e-06]  1663  -0.14   0.25  -0.10   0.14   0.00  0.00\n",
       "1    [1e-07, 1e-05]  2211  -0.30   0.60  -0.20   0.46   1.82  0.00\n",
       "2   [1e-07, 0.0001]  2325  -0.35   0.53  -0.24   0.40   0.43  1.00\n",
       "3    [1e-07, 0.001]  2304  -0.40   0.52  -0.27   0.39   3.03  1.00\n",
       "4     [1e-07, 0.01]   914  -0.49   0.02  -0.34   0.02   3.30  1.00\n",
       "5      [1e-07, 0.1]   465  -0.30   0.26  -0.21   0.18  14.89  7.00\n",
       "6        [1e-07, 1]   395  -0.12   0.11  -0.08   0.03  12.50  7.00\n",
       "7    [1e-06, 1e-05]  2806  -0.21   0.55  -0.13   0.40   1.07  0.00\n",
       "8   [1e-06, 0.0001]  3234  -0.24   0.41  -0.16   0.30   0.62  0.00\n",
       "9    [1e-06, 0.001]  3211  -0.33   0.32  -0.22   0.24   4.67  1.00\n",
       "10    [1e-06, 0.01]  1280  -0.35   0.02  -0.25   0.01   3.12  2.00\n",
       "11     [1e-06, 0.1]   696  -0.22   0.30  -0.15   0.20  14.29  6.00\n",
       "12       [1e-06, 1]   578  -0.09   0.20  -0.06   0.13  12.07  7.00\n",
       "13  [1e-05, 0.0001]  3876  -0.31   0.22  -0.21   0.16   0.52  0.00\n",
       "14   [1e-05, 0.001]  3879  -0.34   0.11  -0.23   0.09   3.87  2.00\n",
       "15    [1e-05, 0.01]  1625  -0.38   0.46  -0.26   0.31   3.55  1.00\n",
       "16     [1e-05, 0.1]   876  -0.40   0.24  -0.29   0.20  10.23  1.00\n",
       "17       [1e-05, 1]   641  -0.19   0.34  -0.13   0.24  10.94  7.00\n",
       "18  [0.0001, 0.001]  3979  -0.35   0.12  -0.23   0.10   4.04  0.00\n",
       "19   [0.0001, 0.01]  1922  -0.36   0.39  -0.25   0.26   3.63  1.00\n",
       "20    [0.0001, 0.1]   918  -0.42   0.17  -0.29   0.14  11.96  1.00\n",
       "21      [0.0001, 1]   663  -0.26   0.35  -0.18   0.24  10.45  7.00\n",
       "22    [0.001, 0.01]  1931  -0.31   0.39  -0.21   0.26   2.59  3.00\n",
       "23     [0.001, 0.1]   913  -0.37   0.21  -0.29   0.17   8.70  2.00\n",
       "24       [0.001, 1]   663  -0.23  -0.20  -0.19  -0.17   7.46  5.00\n",
       "25      [0.01, 0.1]   797  -0.32   0.03  -0.22   0.01  10.00  2.00\n",
       "26        [0.01, 1]   651  -0.26   0.09  -0.17   0.06   9.23  5.00\n",
       "27         [0.1, 1]   639  -0.30   0.25  -0.21   0.17  10.94  7.00"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(table.rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1e-07, 1e-06]</td>\n",
       "      <td>2393</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1e-07, 1e-05]</td>\n",
       "      <td>2391</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1e-07, 0.0001]</td>\n",
       "      <td>2388</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.49</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1e-07, 0.001]</td>\n",
       "      <td>2332</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.37</td>\n",
       "      <td>2.99</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1e-07, 0.01]</td>\n",
       "      <td>1215</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.09</td>\n",
       "      <td>4.20</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[1e-07, 0.1]</td>\n",
       "      <td>585</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.69</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[1e-07, 1]</td>\n",
       "      <td>403</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>12.20</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[1e-06, 1e-05]</td>\n",
       "      <td>3286</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.49</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[1e-06, 0.0001]</td>\n",
       "      <td>3283</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.47</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[1e-06, 0.001]</td>\n",
       "      <td>3226</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>0.33</td>\n",
       "      <td>4.64</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[1e-06, 0.01]</td>\n",
       "      <td>1639</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.09</td>\n",
       "      <td>3.40</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[1e-06, 0.1]</td>\n",
       "      <td>841</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.12</td>\n",
       "      <td>10.71</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[1e-06, 1]</td>\n",
       "      <td>594</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.08</td>\n",
       "      <td>11.67</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[1e-05, 0.0001]</td>\n",
       "      <td>3948</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[1e-05, 0.001]</td>\n",
       "      <td>3892</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.16</td>\n",
       "      <td>3.85</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[1e-05, 0.01]</td>\n",
       "      <td>1900</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.18</td>\n",
       "      <td>2.86</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[1e-05, 0.1]</td>\n",
       "      <td>913</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.25</td>\n",
       "      <td>13.04</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[1e-05, 1]</td>\n",
       "      <td>649</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.25</td>\n",
       "      <td>10.77</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[0.0001, 0.001]</td>\n",
       "      <td>3987</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.14</td>\n",
       "      <td>3.76</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[0.0001, 0.01]</td>\n",
       "      <td>1943</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>0.38</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.25</td>\n",
       "      <td>3.59</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[0.0001, 0.1]</td>\n",
       "      <td>925</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.21</td>\n",
       "      <td>11.83</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[0.0001, 1]</td>\n",
       "      <td>663</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.24</td>\n",
       "      <td>10.45</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[0.001, 0.01]</td>\n",
       "      <td>1936</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.26</td>\n",
       "      <td>2.59</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[0.001, 0.1]</td>\n",
       "      <td>918</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.22</td>\n",
       "      <td>8.70</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[0.001, 1]</td>\n",
       "      <td>663</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>7.46</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[0.01, 0.1]</td>\n",
       "      <td>910</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.02</td>\n",
       "      <td>8.79</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[0.01, 1]</td>\n",
       "      <td>654</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.04</td>\n",
       "      <td>9.09</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[0.1, 1]</td>\n",
       "      <td>652</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.13</td>\n",
       "      <td>10.61</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0     1      2      3      4      5      6     7\n",
       "0    [1e-07, 1e-06]  2393  -0.24   0.32  -0.16   0.22   0.00  0.00\n",
       "1    [1e-07, 1e-05]  2391  -0.22   0.52  -0.15   0.39   1.67  0.00\n",
       "2   [1e-07, 0.0001]  2388  -0.35   0.49  -0.24   0.37   0.42  1.00\n",
       "3    [1e-07, 0.001]  2332  -0.42   0.50  -0.29   0.37   2.99  1.00\n",
       "4     [1e-07, 0.01]  1215  -0.30   0.13  -0.21   0.09   4.20  1.00\n",
       "5      [1e-07, 0.1]   585  -0.49   0.55  -0.36   0.44   1.69  1.00\n",
       "6        [1e-07, 1]   403  -0.12   0.08  -0.08  -0.00  12.20  7.00\n",
       "7    [1e-06, 1e-05]  3286  -0.11   0.49  -0.07   0.36   0.91  0.00\n",
       "8   [1e-06, 0.0001]  3283  -0.24   0.47  -0.16   0.34   0.62  0.00\n",
       "9    [1e-06, 0.001]  3226  -0.33   0.44  -0.23   0.33   4.64  1.00\n",
       "10    [1e-06, 0.01]  1639  -0.26   0.13  -0.18   0.09   3.40  2.00\n",
       "11     [1e-06, 0.1]   841  -0.41   0.13  -0.28   0.12  10.71  1.00\n",
       "12       [1e-06, 1]   594  -0.09   0.13  -0.06   0.08  11.67  7.00\n",
       "13  [1e-05, 0.0001]  3948  -0.32   0.30  -0.21   0.22   0.51  0.00\n",
       "14   [1e-05, 0.001]  3892  -0.35   0.20  -0.24   0.16   3.85  2.00\n",
       "15    [1e-05, 0.01]  1900  -0.32   0.27  -0.22   0.18   2.86  1.00\n",
       "16     [1e-05, 0.1]   913  -0.38   0.31  -0.28   0.25  13.04  1.00\n",
       "17       [1e-05, 1]   649  -0.18   0.36  -0.12   0.25  10.77  7.00\n",
       "18  [0.0001, 0.001]  3987  -0.35   0.18  -0.24   0.14   3.76  0.00\n",
       "19   [0.0001, 0.01]  1943  -0.37   0.38  -0.26   0.25   3.59  1.00\n",
       "20    [0.0001, 0.1]   925  -0.43   0.26  -0.30   0.21  11.83  1.00\n",
       "21      [0.0001, 1]   663  -0.26   0.35  -0.18   0.24  10.45  7.00\n",
       "22    [0.001, 0.01]  1936  -0.32   0.40  -0.21   0.26   2.59  3.00\n",
       "23     [0.001, 0.1]   918  -0.38   0.27  -0.30   0.22   8.70  2.00\n",
       "24       [0.001, 1]   663  -0.23  -0.20  -0.19  -0.17   7.46  5.00\n",
       "25      [0.01, 0.1]   910  -0.36   0.04  -0.25   0.02   8.79  2.00\n",
       "26        [0.01, 1]   654  -0.25   0.06  -0.16   0.04   9.09  5.00\n",
       "27         [0.1, 1]   652  -0.27   0.20  -0.19   0.13  10.61  7.00"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(table.rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Weights</th>\n",
       "            <th>Archs</th>\n",
       "            <th>Overall Spearman</th>\n",
       "            <th>Top-10% Spearman</th>\n",
       "            <th>Top-10%/top-10%</th>\n",
       "            <th>Top-64/top-5%</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>[1e-07, 1e-06]</td>\n",
       "            <td>484</td>\n",
       "            <td>-0.21<br></td>\n",
       "            <td>-0.20<br></td>\n",
       "            <td>4.35<br></td>\n",
       "            <td>3.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[1e-07, 1e-05]</td>\n",
       "            <td>484</td>\n",
       "            <td>-0.27<br></td>\n",
       "            <td>-0.08<br></td>\n",
       "            <td>31.25<br></td>\n",
       "            <td>6.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[1e-07, 0.0001]</td>\n",
       "            <td>482</td>\n",
       "            <td>-0.34<br></td>\n",
       "            <td>-0.03<br></td>\n",
       "            <td>28.89<br></td>\n",
       "            <td>7.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[1e-07, 0.001]</td>\n",
       "            <td>471</td>\n",
       "            <td>-0.44<br></td>\n",
       "            <td>0.14<br></td>\n",
       "            <td>34.04<br></td>\n",
       "            <td>7.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[1e-07, 0.01]</td>\n",
       "            <td>245</td>\n",
       "            <td>-0.33<br></td>\n",
       "            <td>-0.12<br></td>\n",
       "            <td>47.62<br></td>\n",
       "            <td>8.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[1e-07, 0.1]</td>\n",
       "            <td>123</td>\n",
       "            <td>-0.40<br></td>\n",
       "            <td>0.14<br></td>\n",
       "            <td>30.77<br></td>\n",
       "            <td>7.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[1e-07, 1]</td>\n",
       "            <td>90</td>\n",
       "            <td>0.01<br></td>\n",
       "            <td>-0.32<br></td>\n",
       "            <td>11.11<br></td>\n",
       "            <td>4.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[1e-06, 1e-05]</td>\n",
       "            <td>642</td>\n",
       "            <td>-0.09<br></td>\n",
       "            <td>-0.09<br></td>\n",
       "            <td>16.92<br></td>\n",
       "            <td>6.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[1e-06, 0.0001]</td>\n",
       "            <td>642</td>\n",
       "            <td>-0.23<br></td>\n",
       "            <td>0.01<br></td>\n",
       "            <td>25.42<br></td>\n",
       "            <td>9.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[1e-06, 0.001]</td>\n",
       "            <td>628</td>\n",
       "            <td>-0.36<br></td>\n",
       "            <td>-0.13<br></td>\n",
       "            <td>28.57<br></td>\n",
       "            <td>8.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[1e-06, 0.01]</td>\n",
       "            <td>326</td>\n",
       "            <td>-0.29<br></td>\n",
       "            <td>-0.17<br></td>\n",
       "            <td>45.83<br></td>\n",
       "            <td>8.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[1e-06, 0.1]</td>\n",
       "            <td>172</td>\n",
       "            <td>-0.37<br></td>\n",
       "            <td>0.31<br></td>\n",
       "            <td>27.78<br></td>\n",
       "            <td>5.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[1e-06, 1]</td>\n",
       "            <td>121</td>\n",
       "            <td>0.08<br></td>\n",
       "            <td>-0.17<br></td>\n",
       "            <td>16.67<br></td>\n",
       "            <td>3.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[1e-05, 0.0001]</td>\n",
       "            <td>789</td>\n",
       "            <td>-0.32<br></td>\n",
       "            <td>-0.19<br></td>\n",
       "            <td>25.32<br></td>\n",
       "            <td>11.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[1e-05, 0.001]</td>\n",
       "            <td>776</td>\n",
       "            <td>-0.37<br></td>\n",
       "            <td>-0.29<br></td>\n",
       "            <td>26.92<br></td>\n",
       "            <td>11.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[1e-05, 0.01]</td>\n",
       "            <td>387</td>\n",
       "            <td>-0.34<br></td>\n",
       "            <td>-0.22<br></td>\n",
       "            <td>39.47<br></td>\n",
       "            <td>9.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[1e-05, 0.1]</td>\n",
       "            <td>187</td>\n",
       "            <td>-0.37<br></td>\n",
       "            <td>0.20<br></td>\n",
       "            <td>31.58<br></td>\n",
       "            <td>6.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[1e-05, 1]</td>\n",
       "            <td>136</td>\n",
       "            <td>-0.03<br></td>\n",
       "            <td>-0.26<br></td>\n",
       "            <td>14.29<br></td>\n",
       "            <td>4.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[0.0001, 0.001]</td>\n",
       "            <td>796</td>\n",
       "            <td>-0.38<br></td>\n",
       "            <td>-0.22<br></td>\n",
       "            <td>22.78<br></td>\n",
       "            <td>7.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[0.0001, 0.01]</td>\n",
       "            <td>400</td>\n",
       "            <td>-0.43<br></td>\n",
       "            <td>-0.31<br></td>\n",
       "            <td>43.59<br></td>\n",
       "            <td>12.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[0.0001, 0.1]</td>\n",
       "            <td>190</td>\n",
       "            <td>-0.39<br></td>\n",
       "            <td>0.18<br></td>\n",
       "            <td>26.32<br></td>\n",
       "            <td>3.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[0.0001, 1]</td>\n",
       "            <td>139</td>\n",
       "            <td>-0.13<br></td>\n",
       "            <td>-0.34<br></td>\n",
       "            <td>14.29<br></td>\n",
       "            <td>4.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[0.001, 0.01]</td>\n",
       "            <td>399</td>\n",
       "            <td>-0.34<br></td>\n",
       "            <td>-0.37<br></td>\n",
       "            <td>35.00<br></td>\n",
       "            <td>11.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[0.001, 0.1]</td>\n",
       "            <td>189</td>\n",
       "            <td>-0.27<br></td>\n",
       "            <td>-0.19<br></td>\n",
       "            <td>21.05<br></td>\n",
       "            <td>2.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[0.001, 1]</td>\n",
       "            <td>139</td>\n",
       "            <td>-0.13<br></td>\n",
       "            <td>-0.25<br></td>\n",
       "            <td>0.00<br></td>\n",
       "            <td>2.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[0.01, 0.1]</td>\n",
       "            <td>185</td>\n",
       "            <td>-0.33<br></td>\n",
       "            <td>0.10<br></td>\n",
       "            <td>37.50<br></td>\n",
       "            <td>3.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[0.01, 1]</td>\n",
       "            <td>137</td>\n",
       "            <td>-0.26<br></td>\n",
       "            <td>0.06<br></td>\n",
       "            <td>28.57<br></td>\n",
       "            <td>3.00<br></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>[0.1, 1]</td>\n",
       "            <td>137</td>\n",
       "            <td>-0.21<br></td>\n",
       "            <td>0.37<br></td>\n",
       "            <td>14.29<br></td>\n",
       "            <td>4.00<br></td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+-----------------+-------+------------------+------------------+-----------------+---------------+\n",
       "| Weights         | Archs | Overall Spearman | Top-10% Spearman | Top-10%/top-10% | Top-64/top-5% |\n",
       "+-----------------+-------+------------------+------------------+-----------------+---------------+\n",
       "| [1e-07, 1e-06]  | 484   | -0.21            | -0.20            | 4.35            | 3.00          |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [1e-07, 1e-05]  | 484   | -0.27            | -0.08            | 31.25           | 6.00          |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [1e-07, 0.0001] | 482   | -0.34            | -0.03            | 28.89           | 7.00          |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [1e-07, 0.001]  | 471   | -0.44            | 0.14             | 34.04           | 7.00          |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [1e-07, 0.01]   | 245   | -0.33            | -0.12            | 47.62           | 8.00          |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [1e-07, 0.1]    | 123   | -0.40            | 0.14             | 30.77           | 7.00          |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [1e-07, 1]      | 90    | 0.01             | -0.32            | 11.11           | 4.00          |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [1e-06, 1e-05]  | 642   | -0.09            | -0.09            | 16.92           | 6.00          |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [1e-06, 0.0001] | 642   | -0.23            | 0.01             | 25.42           | 9.00          |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [1e-06, 0.001]  | 628   | -0.36            | -0.13            | 28.57           | 8.00          |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [1e-06, 0.01]   | 326   | -0.29            | -0.17            | 45.83           | 8.00          |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [1e-06, 0.1]    | 172   | -0.37            | 0.31             | 27.78           | 5.00          |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [1e-06, 1]      | 121   | 0.08             | -0.17            | 16.67           | 3.00          |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [1e-05, 0.0001] | 789   | -0.32            | -0.19            | 25.32           | 11.00         |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [1e-05, 0.001]  | 776   | -0.37            | -0.29            | 26.92           | 11.00         |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [1e-05, 0.01]   | 387   | -0.34            | -0.22            | 39.47           | 9.00          |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [1e-05, 0.1]    | 187   | -0.37            | 0.20             | 31.58           | 6.00          |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [1e-05, 1]      | 136   | -0.03            | -0.26            | 14.29           | 4.00          |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [0.0001, 0.001] | 796   | -0.38            | -0.22            | 22.78           | 7.00          |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [0.0001, 0.01]  | 400   | -0.43            | -0.31            | 43.59           | 12.00         |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [0.0001, 0.1]   | 190   | -0.39            | 0.18             | 26.32           | 3.00          |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [0.0001, 1]     | 139   | -0.13            | -0.34            | 14.29           | 4.00          |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [0.001, 0.01]   | 399   | -0.34            | -0.37            | 35.00           | 11.00         |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [0.001, 0.1]    | 189   | -0.27            | -0.19            | 21.05           | 2.00          |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [0.001, 1]      | 139   | -0.13            | -0.25            | 0.00            | 2.00          |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [0.01, 0.1]     | 185   | -0.33            | 0.10             | 37.50           | 3.00          |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [0.01, 1]       | 137   | -0.26            | 0.06             | 28.57           | 3.00          |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "| [0.1, 1]        | 137   | -0.21            | 0.37             | 14.29           | 4.00          |\n",
       "|                 |       |                  |                  |                 |               |\n",
       "+-----------------+-------+------------------+------------------+-----------------+---------------+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_names = [\"uniform_positive_0.1\",\n",
    "                   \"uniform_positive_1\",\n",
    "                   \"uniform_centered_0.1\",\n",
    "                   \"uniform_centered_1\",\n",
    "                   \"random_0.1\",\n",
    "                   \"random_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_l = 1e-5\n",
    "weight_h = 1e-3\n",
    "it = 0\n",
    "for embedding_name in embedding_names:\n",
    "    save_dir = '{}/NLP/Ablation/Embedding/Embed_{}/'.format(results_dir, embedding_name)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    if os.path.exists(save_dir+'Data'):\n",
    "        # Load precomputed results\n",
    "        data_file = open(save_dir+'Data','rb')\n",
    "        data = pkl.load(data_file)\n",
    "        score = data[\"score\"]\n",
    "        accs = data[\"accs\"]\n",
    "        nparams = data[\"nparams\"]\n",
    "    else:\n",
    "        accs = []\n",
    "        nparams = []\n",
    "        score = []\n",
    "        for i in trange(1000):\n",
    "            file = file_list[i]\n",
    "            log = json.load(open('train_logs_single_run/' + file, 'r'))\n",
    "            args = Namespace(**log)\n",
    "\n",
    "            # Build the model\n",
    "            network = AWDRNNModel(args.model,\n",
    "                                  ntokens,\n",
    "                                  args.emsize,\n",
    "                                  args.nhid,\n",
    "                                  args.nlayers,\n",
    "                                  args.dropout,\n",
    "                                  args.dropouth,\n",
    "                                  args.dropouti,\n",
    "                                  args.dropoute,\n",
    "                                  args.wdrop,\n",
    "                                  args.tied,\n",
    "                                  args.recepie,\n",
    "                                  verbose=False)\n",
    "            preds = []\n",
    "            for weight in [weight_l, weight_h]:\n",
    "                # Initialize\n",
    "                prepare_seed(21)\n",
    "                def initialize_resnet(m):\n",
    "                    if type(m)==MultiLinear:\n",
    "                        for par in m.weights_raw:\n",
    "                            nn.init.constant_(par, weight)\n",
    "                    elif type(m)==CustomRNNCell:\n",
    "                        for par in m.parameters():\n",
    "                            nn.init.constant_(par, weight)\n",
    "                    elif type(m)==nn.modules.linear.Linear:\n",
    "                        nn.init.constant_(m.weight, weight)\n",
    "                    elif type(m)==nn.modules.container.ParameterList:\n",
    "                        for par in m.parameters():\n",
    "                            nn.init.constant_(par, weight)\n",
    "                    elif type(m)==CustomRNN:\n",
    "                        initialize_resnet(m.cell)\n",
    "                    elif type(m)==ParameterListWeightDrop:\n",
    "                        initialize_resnet(m.module)\n",
    "                    elif type(m)==nn.modules.container.ModuleDict:\n",
    "                        for sub_m in m:\n",
    "                            initialize_resnet(sub_m)\n",
    "                    elif type(m)==nn.modules.container.ModuleList:\n",
    "                        for sub_m in m:\n",
    "                            initialize_resnet(sub_m)\n",
    "                    elif type(m)==AWDRNNModel:\n",
    "                        initialize_resnet(m.rnns)\n",
    "\n",
    "                network.apply(initialize_resnet)\n",
    "                network.eval()\n",
    "                hidden = network.init_hidden(batch_size, weight)\n",
    "                \n",
    "                # Initialise embedding\n",
    "                if embedding_name==\"uniform_positive_0.1\":\n",
    "                    nn.init.uniform_(network.encoder.weight, 0, 0.1)\n",
    "                elif embedding_name==\"uniform_positive_1\":\n",
    "                    nn.init.uniform_(network.encoder.weight, 0, 1)\n",
    "                elif embedding_name==\"uniform_centered_0.1\":\n",
    "                    nn.init.uniform_(network.encoder.weight, -0.1, 0.1)\n",
    "                elif embedding_name==\"uniform_centered_1\":\n",
    "                    nn.init.uniform_(network.encoder.weight, -1, 1)\n",
    "                elif embedding_name==\"random_0.1\":\n",
    "                    nn.init.normal_(network.encoder.weight, mean=0.0, std=0.1)\n",
    "                elif embedding_name==\"random_1\":\n",
    "                    nn.init.normal_(network.encoder.weight, mean=0.0, std=1)\n",
    "                    \n",
    "                _, _, raw_output, _ = network(x, hidden=hidden, return_h=True)\n",
    "                pred = raw_output[-1][:,:,0].flatten()\n",
    "                pred = pred.numpy()\n",
    "                pred_min = np.nanmin(pred)\n",
    "                pred_max = np.nanmax(pred)\n",
    "                pred_norm = (pred - pred_min)/(pred_max - pred_min)\n",
    "                preds.append(pred_norm)\n",
    "\n",
    "            # Compute the score\n",
    "            preds = np.array(preds)\n",
    "            preds[np.where(preds==0)] = np.nan\n",
    "            mae = np.nanmean(np.abs(preds[0,:]-preds[1,:]))\n",
    "            mean = np.nanmean(preds)\n",
    "\n",
    "            score.append(mae/mean)\n",
    "            nparams.append(args.num_params)\n",
    "            try:\n",
    "                accs.append(log['test_losses'][-1])\n",
    "            except:\n",
    "                accs.append(np.nan) \n",
    "\n",
    "        save_dic = {}\n",
    "        save_dic[\"score\"] = score\n",
    "        save_dic[\"accs\"] = accs\n",
    "        save_dic[\"nparams\"] = nparams\n",
    "        pkl.dump(save_dic, open(save_dir + \"Data\", \"wb\"))\n",
    "\n",
    "    if it==0:\n",
    "        # Make the table\n",
    "        headers = [\"Data\", \"Archs\", \"Spearman (global)\", \"Spearman (top-10%)\", \"Kendall (global)\", \"Kendall (top-10%)\", \"Top-10%/top-10%\", \"Top-64/top-5%\"]\n",
    "        table = PrettyTable(headers, align='l')\n",
    "    stats, remain = compute_stats(score, accs)\n",
    "    stats_print = [embedding_name, remain] + stats       \n",
    "    table.add_row(stats_print)\n",
    "    it+=1\n",
    "\n",
    "#     plot_results(score=score,\n",
    "#                  accs=accs,\n",
    "#                  nparams=nparams,\n",
    "#                  top10=False,\n",
    "#                  log_scale=True,\n",
    "#                  save_dir=save_dir,\n",
    "#                  save_name=\"Epsilon_{}\".format(embedding_name))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>uniform_positive_0.1</td>\n",
       "      <td>782</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3.80</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>uniform_positive_1</td>\n",
       "      <td>776</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.18</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uniform_centered_0.1</td>\n",
       "      <td>783</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2.53</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>uniform_centered_1</td>\n",
       "      <td>782</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>random_0.1</td>\n",
       "      <td>783</td>\n",
       "      <td>-0.42</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2.53</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>random_1</td>\n",
       "      <td>782</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0    1      2     3      4     5     6     7\n",
       "0  uniform_positive_0.1  782  -0.38  0.17  -0.26  0.15  3.80  2.00\n",
       "1    uniform_positive_1  776  -0.37  0.21  -0.26  0.18  2.56  2.00\n",
       "2  uniform_centered_0.1  783  -0.40  0.18  -0.28  0.15  2.53  2.00\n",
       "3    uniform_centered_1  782  -0.46  0.18  -0.33  0.16  1.27  1.00\n",
       "4            random_0.1  783  -0.42  0.18  -0.29  0.15  2.53  2.00\n",
       "5              random_1  782  -0.47  0.19  -0.33  0.16  1.27  1.00"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(table.rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|                                        | 6/1000 [00:03<10:52,  1.52it/s]/home/gracheva/miniconda3/envs/tf2/lib/python3.7/site-packages/ipykernel_launcher.py:100: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/gracheva/miniconda3/envs/tf2/lib/python3.7/site-packages/ipykernel_launcher.py:106: RuntimeWarning: Mean of empty slice\n",
      "  3%|                                      | 32/1000 [00:18<08:41,  1.86it/s]/home/gracheva/miniconda3/envs/tf2/lib/python3.7/site-packages/ipykernel_launcher.py:100: RuntimeWarning: invalid value encountered in subtract\n",
      " 67%|            | 672/1000 [06:17<03:05,  1.76it/s]/home/gracheva/miniconda3/envs/tf2/lib/python3.7/site-packages/ipykernel_launcher.py:107: RuntimeWarning: Mean of empty slice\n",
      "100%|| 1000/1000 [09:19<00:00,  1.79it/s]\n",
      "100%|| 1000/1000 [09:18<00:00,  1.79it/s]\n",
      "100%|| 1000/1000 [09:13<00:00,  1.81it/s]\n",
      "100%|| 1000/1000 [09:58<00:00,  1.67it/s]\n",
      "100%|| 1000/1000 [09:54<00:00,  1.68it/s]\n",
      "100%|| 1000/1000 [25:45<00:00,  1.55s/it]\n",
      "100%|| 1000/1000 [25:45<00:00,  1.55s/it]\n",
      "100%|| 1000/1000 [24:14<00:00,  1.45s/it]\n",
      "100%|| 1000/1000 [24:12<00:00,  1.45s/it]\n",
      "100%|| 1000/1000 [24:11<00:00,  1.45s/it]\n",
      "100%|| 1000/1000 [25:54<00:00,  1.55s/it]\n",
      "100%|| 1000/1000 [25:38<00:00,  1.54s/it]\n",
      "100%|| 1000/1000 [25:24<00:00,  1.52s/it]\n",
      " 68%|            | 675/1000 [37:44<21:25,  3.95s/it]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|| 1000/1000 [55:25<00:00,  3.33s/it]\n",
      "100%|| 1000/1000 [55:00<00:00,  3.30s/it]\n",
      " 73%|          | 728/1000 [41:13<15:24,  3.40s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at CPUAllocator.cpp:65] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 86016000 bytes. Error code 12 (Cannot allocate memory)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35506/2093657860.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_h\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/NAS/NAS-Bench-NLP/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, return_h)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mraw_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0mraw_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlayers\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/NAS/NAS-Bench-NLP/weight_drop.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setweights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Work/NAS/NAS-Bench-NLP/custom_rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, hidden_tuple)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhidden_tuple\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at CPUAllocator.cpp:65] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 86016000 bytes. Error code 12 (Cannot allocate memory)"
     ]
    }
   ],
   "source": [
    "batch_size_range = [8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "\n",
    "weight_l = 1e-5\n",
    "weight_h = 1e-3\n",
    "it = 0\n",
    "\n",
    "batch_all_spearman_all = []\n",
    "batch_top10_spearman_all = []\n",
    "batch_all_kendall_all = []\n",
    "batch_top10_kendall_all = []\n",
    "batch_top10top10_all = []\n",
    "batch_top64top5_all = []\n",
    "\n",
    "for batch_size in batch_size_range:\n",
    "    batch_all_spearman = []\n",
    "    batch_top10_spearman = []\n",
    "    batch_all_kendall = []\n",
    "    batch_top10_kendall = []\n",
    "    batch_top10top10 = []\n",
    "    batch_top64top5 = []\n",
    "    \n",
    "    for it in range(3):\n",
    "        # Load the data batch\n",
    "        if it==0:\n",
    "            train_eval_data = batchify(corpus.train, batch_size, args, \"cpu\")\n",
    "            ind = np.random.choice(range(len(train_eval_data)-args.bptt), 5, replace=False)\n",
    "        x, _ = get_batch(train_eval_data, ind[it], args, evaluation=True)\n",
    "\n",
    "        save_dir = '{}/NLP/Ablation/Batch Size/BS{}/'.format(results_dir, batch_size)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        if os.path.exists(save_dir+'Data_' + str(it)):\n",
    "            # Load precomputed results\n",
    "            data_file = open(save_dir + 'Data_' + str(it),'rb')\n",
    "            input_data = pkl.load(data_file)\n",
    "            score = input_data[\"score\"]\n",
    "            accs = input_data[\"accs\"]\n",
    "            nparams = input_data[\"nparams\"]\n",
    "        else:\n",
    "            accs = []\n",
    "            nparams = []\n",
    "            score = []\n",
    "            for i in trange(1000):\n",
    "                file = file_list[i]\n",
    "                log = json.load(open('train_logs_single_run/' + file, 'r'))\n",
    "                args = Namespace(**log)\n",
    "\n",
    "                # Build the model\n",
    "                network = AWDRNNModel(args.model,\n",
    "                                      ntokens,\n",
    "                                      args.emsize,\n",
    "                                      args.nhid,\n",
    "                                      args.nlayers,\n",
    "                                      args.dropout,\n",
    "                                      args.dropouth,\n",
    "                                      args.dropouti,\n",
    "                                      args.dropoute,\n",
    "                                      args.wdrop,\n",
    "                                      args.tied,\n",
    "                                      args.recepie,\n",
    "                                      verbose=False)\n",
    "                preds = []\n",
    "                for weight in [weight_l, weight_h]:\n",
    "                    # Initialize\n",
    "                    prepare_seed(21)\n",
    "                    def initialize_resnet(m):\n",
    "                        if type(m)==MultiLinear:\n",
    "                            for par in m.weights_raw:\n",
    "                                nn.init.constant_(par, weight)\n",
    "                        elif type(m)==CustomRNNCell:\n",
    "                            for par in m.parameters():\n",
    "                                nn.init.constant_(par, weight)\n",
    "                        elif type(m)==nn.modules.linear.Linear:\n",
    "                            nn.init.constant_(m.weight, weight)\n",
    "                        elif type(m)==nn.modules.container.ParameterList:\n",
    "                            for par in m.parameters():\n",
    "                                nn.init.constant_(par, weight)\n",
    "                        elif type(m)==CustomRNN:\n",
    "                            initialize_resnet(m.cell)\n",
    "                        elif type(m)==ParameterListWeightDrop:\n",
    "                            initialize_resnet(m.module)\n",
    "                        elif type(m)==nn.modules.container.ModuleDict:\n",
    "                            for sub_m in m:\n",
    "                                initialize_resnet(sub_m)\n",
    "                        elif type(m)==nn.modules.container.ModuleList:\n",
    "                            for sub_m in m:\n",
    "                                initialize_resnet(sub_m)\n",
    "                        elif type(m)==AWDRNNModel:\n",
    "                            initialize_resnet(m.rnns)\n",
    "\n",
    "                    network.apply(initialize_resnet)\n",
    "                    network.eval()\n",
    "                    hidden = network.init_hidden(batch_size, weight)\n",
    "                    nn.init.uniform_(network.encoder.weight, 0, 1)\n",
    "                    _, _, raw_output, _ = network(x, hidden=hidden, return_h=True)\n",
    "                    pred = raw_output[-1][:,:,0].flatten()\n",
    "                    pred = pred.numpy()\n",
    "                    pred_min = np.nanmin(pred)\n",
    "                    pred_max = np.nanmax(pred)\n",
    "                    pred_norm = (pred - pred_min)/(pred_max - pred_min)\n",
    "                    preds.append(pred_norm)\n",
    "\n",
    "                # Compute the score\n",
    "                preds = np.array(preds)\n",
    "                preds[np.where(preds==0)] = np.nan\n",
    "                mae = np.nanmean(np.abs(preds[0,:]-preds[1,:]))\n",
    "                mean = np.nanmean(preds)\n",
    "\n",
    "                score.append(mae/mean)\n",
    "                nparams.append(args.num_params)\n",
    "                try:\n",
    "                    accs.append(log['test_losses'][-1])\n",
    "                except:\n",
    "                    accs.append(np.nan)\n",
    "\n",
    "            save_dic = {}\n",
    "            save_dic[\"score\"] = score\n",
    "            save_dic[\"accs\"] = accs\n",
    "            save_dic[\"nparams\"] = nparams\n",
    "            pkl.dump(save_dic, open(save_dir + \"Data_\" + str(it), \"wb\"))\n",
    "\n",
    "        stats, _ = compute_stats(score, accs, reverse=True, raw=True)\n",
    "        batch_all_spearman.append(stats[0])\n",
    "        batch_top10_spearman.append(stats[1])\n",
    "        batch_all_kendall.append(stats[2])\n",
    "        batch_top10_kendall.append(stats[3])\n",
    "        batch_top10top10.append(stats[4])\n",
    "        batch_top64top5.append(stats[5])\n",
    "\n",
    "    batch_all_spearman_all.append(batch_all_spearman)\n",
    "    batch_top10_spearman_all.append(batch_top10_spearman)\n",
    "    batch_all_kendall_all.append(batch_all_kendall)\n",
    "    batch_top10_kendall_all.append(batch_top10_kendall)\n",
    "    batch_top10top10_all.append(batch_top10top10)\n",
    "    batch_top64top5_all.append(batch_top64top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "def plot_experiment(exp_list, filename):\n",
    "    fig = plt.figure(figsize=(7.2,4.45))\n",
    "    ax = fig.add_subplot(111)\n",
    "    def plot_exp(exp, label):\n",
    "        exp = np.array(exp)\n",
    "        q_75 = np.nanquantile(exp, .75, axis=1)\n",
    "        q_25 = np.nanquantile(exp, .25, axis=1)\n",
    "        mean = np.nanmedian(exp, axis=1)\n",
    "        ax.plot(range(len(mean)), mean, label=label)\n",
    "        ax.fill_between(range(len(mean)), np.nanmin(exp, axis=1), np.nanmax(exp, axis=1), alpha=0.1)\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        ax.set_xticklabels(['','8','16','32'])\n",
    "#         ax.set_xticklabels(['','8','16','32','64','128','256','512','1024'])\n",
    "        for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "                     ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "            item.set_fontsize(16)\n",
    "    for exp,ename in exp_list:\n",
    "        plot_exp(exp,ename)\n",
    "    plt.grid()\n",
    "    plt.xlabel('Batch size', fontsize=18)\n",
    "    plt.ylabel(r'Spearman $\\rho$', fontsize=18)\n",
    "#     plt.ylim(0.3,1)\n",
    "    plt.legend(fontsize=16)\n",
    "    plt.savefig(filename,\n",
    "                bbox_inches='tight', \n",
    "                dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gracheva/miniconda3/envs/tf2/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 518.4x320.4 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_dir = './release_results/NLP/Ablation/Batch Size/'\n",
    "filename = save_dir+\"BatchSize_Spearman_NLP_Single.png\"\n",
    "plot_experiment([(batch_all_spearman_all,'Overall'), (batch_top10_spearman_all,'Top-10%')], filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
